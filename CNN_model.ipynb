{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjB5msgp4ohG"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect device\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(f\"💻 GPU is available: {device_name}\")\n",
        "else:\n",
        "    print(\"⚠️ GPU not found. Using CPU...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfPLXWSL7qzo",
        "outputId": "1403eae3-c4b7-4539-d69d-a9b762bdedd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💻 GPU is available: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Smaller CNN (4 conv layers)\n",
        "def build_small_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "VSR9tjJk8aPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader and preprocessor\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def prepare_dataset(name):\n",
        "    if name == \"mnist\":\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "        x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    elif name == \"fashion_mnist\":\n",
        "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "        x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    elif name == \"cifar10\":\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset\")\n",
        "\n",
        "    # Normalize and one-hot encode\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    y_train = to_categorical(y_train, 10)\n",
        "    y_test = to_categorical(y_test, 10)\n",
        "\n",
        "    # Reduce data for faster CPU training\n",
        "    x_train = x_train[:10000]\n",
        "    y_train = y_train[:10000]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test, x_train.shape[1:]\n"
      ],
      "metadata": {
        "id": "24hmPE2z81MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "import time\n",
        "\n",
        "def train_model(dataset_name, epochs_list=[5, 10]):\n",
        "    print(f\"\\n📦 Dataset: {dataset_name}\")\n",
        "\n",
        "    # Corrected line split\n",
        "    x_train, y_train, x_test, y_test, input_shape = prepare_dataset(dataset_name)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for epochs in epochs_list:\n",
        "        model = build_small_cnn_model(input_shape, 10)\n",
        "        start = time.time()\n",
        "        history = model.fit(\n",
        "            x_train, y_train,\n",
        "            epochs=epochs,\n",
        "            batch_size=64,\n",
        "            validation_split=0.1,\n",
        "            verbose=0\n",
        "        )\n",
        "        end = time.time()\n",
        "\n",
        "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        duration = end - start\n",
        "\n",
        "        print(f\"🧪 Epochs: {epochs} | Accuracy: {test_acc:.4f} | Time: {duration:.2f}s\")\n",
        "        results.append((epochs, test_acc, duration))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "I4gNlgv_9M_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run for each dataset\n",
        "all_results = {}\n",
        "datasets = [\"mnist\", \"fashion_mnist\"]\n",
        "\n",
        "for name in datasets:\n",
        "    all_results[name] = train_model(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmMxYoBL9cXm",
        "outputId": "45002cc4-aa85-4cf3-91f8-033bc12342db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📦 Dataset: mnist\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Epochs: 5 | Accuracy: 0.9748 | Time: 14.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Epochs: 10 | Accuracy: 0.9830 | Time: 12.20s\n",
            "\n",
            "📦 Dataset: fashion_mnist\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Epochs: 5 | Accuracy: 0.8630 | Time: 8.68s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Epochs: 10 | Accuracy: 0.8774 | Time: 12.94s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print final summary\n",
        "print(\"\\n✅ Summary (GPU Optimized):\")\n",
        "print(\"Dataset\\t\\tEpochs\\tAccuracy\\tTime (s)\")\n",
        "\n",
        "for dataset, records in all_results.items():\n",
        "    for epochs, acc, dur in records:\n",
        "        print(f\"{dataset}\\t{epochs}\\t{acc:.4f}\\t\\t{dur:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-X8FvID9kQG",
        "outputId": "965731fc-c84c-497c-cf1d-1bb877f24ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Summary (GPU Optimized):\n",
            "Dataset\t\tEpochs\tAccuracy\tTime (s)\n",
            "mnist\t5\t0.9748\t\t14.55\n",
            "mnist\t10\t0.9830\t\t12.20\n",
            "fashion_mnist\t5\t0.8630\t\t8.68\n",
            "fashion_mnist\t10\t0.8774\t\t12.94\n"
          ]
        }
      ]
    }
  ]
}